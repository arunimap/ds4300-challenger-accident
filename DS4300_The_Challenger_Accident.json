{"paragraphs":[{"text":"// Read temperature data for 1986 and station data\nval temps = spark.read.csv(\"/Users/arunima/Desktop/ArunimaMAC/Education/Northeastern/Year_3/ds4300/HW/ds4300-challenger-accident/temperatures/1986.csv\").toDF(\"STATION\", \"WBAN\", \"MONTH\", \"DAY\", \"TEMP\")\nval stations = spark.read.csv(\"/Users/arunima/Desktop/ArunimaMAC/Education/Northeastern/Year_3/ds4300/HW/ds4300-challenger-accident/temperatures/stations.csv\").toDF(\"STATION\", \"WBAN\", \"LAT\", \"LON\")","user":"anonymous","dateUpdated":"2020-04-08T17:07:07-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"temps: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 3 more fields]\nstations: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1586286925674_-1241852641","id":"20180318-191454_284618957","dateCreated":"2020-04-07T15:15:25-0400","dateStarted":"2020-04-08T17:07:07-0400","dateFinished":"2020-04-08T17:07:09-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3082"},{"text":"// Filter bad Stations where the latitude or longitude is unavailable, convert temp, long, and lat to doubles\n// Filtered only to include temperature data for JANUARY 28th, 1986\n\nval stations_clean = stations.filter(($\"LAT\".isNotNull) && ($\"LON\".isNotNull)).selectExpr(\"STATION\", \"WBAN\", \"cast(LAT as double) LAT\", \"cast(LON as double) LON\")\nval temps_0128 = temps.filter(($\"MONTH\" === \"01\") && ($\"DAY\" === \"28\")).selectExpr(\"STATION\", \"WBAN\", \"MONTH\", \"DAY\", \"cast(TEMP as double)\")","user":"anonymous","dateUpdated":"2020-04-08T17:55:20-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"stations_clean: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 2 more fields]\ntemps_0128: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1586286925676_-1851971540","id":"20180318-191924_1310538952","dateCreated":"2020-04-07T15:15:25-0400","dateStarted":"2020-04-08T17:07:09-0400","dateFinished":"2020-04-08T17:07:10-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3083"},{"text":"// Define a function that compute the distance between two points on the Earth using the Haversine formula\n// https://www.movable-type.co.uk/scripts/latlong.html\n// Declare the function as a UDF (User-defined function) so that it can be applied\n\nval pi = 3.14159265\nval REarth = 6371.0 // kilometers\ndef toRadians(x: Double): Double = x * pi / 180.0\n\ndef haversine(lat1: Double, lon1: Double, lat2: Double, lon2: Double): Double = {\n    var φ1 = toRadians(lat1);\n    var φ2 = toRadians(lat2);\n    var Δφ = toRadians(lat2-lat1);\n    var Δλ = toRadians(lon2-lon1);\n\n    var a = Math.sin(Δφ/2) * Math.sin(Δφ/2) +\n            Math.cos(φ1) * Math.cos(φ2) *\n            Math.sin(Δλ/2) * Math.sin(Δλ/2);\n    var c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n\n    REarth * c;\n}\n\n\n// Now you can use \"haver\" as a function with Spark SQL \nimport org.apache.spark.sql.functions.udf\nval haver = udf(haversine _)","user":"anonymous","dateUpdated":"2020-04-08T17:07:10-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"pi: Double = 3.14159265\nREarth: Double = 6371.0\ntoRadians: (x: Double)Double\nhaversine: (lat1: Double, lon1: Double, lat2: Double, lon2: Double)Double\nimport org.apache.spark.sql.functions.udf\nhaver: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function4>,DoubleType,Some(List(DoubleType, DoubleType, DoubleType, DoubleType)))\n"}]},"apps":[],"jobName":"paragraph_1586286925677_-1437940270","id":"20180318-195342_1996330025","dateCreated":"2020-04-07T15:15:25-0400","dateStarted":"2020-04-08T17:07:10-0400","dateFinished":"2020-04-08T17:07:11-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3084"},{"text":"// Find all stations within 100 km using your haversine function\n\nval capeCanaveralLatitude = 28.388382\nval capeCanaveralLongitude = -80.603498\n\nval stations_clean_near = stations_clean.withColumn(\"CapeCanaveralDist\", haver($\"LAT\", $\"LON\", lit(capeCanaveralLatitude), lit(capeCanaveralLongitude))).filter($\"CapeCanaveralDist\" < 100)","user":"anonymous","dateUpdated":"2020-04-08T17:07:11-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"capeCanaveralLatitude: Double = 28.388382\ncapeCanaveralLongitude: Double = -80.603498\nstations_clean_near: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STATION: string, WBAN: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1586286925678_1759852908","id":"20180318-191955_1910278437","dateCreated":"2020-04-07T15:15:25-0400","dateStarted":"2020-04-08T17:07:11-0400","dateFinished":"2020-04-08T17:07:11-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3085"},{"text":"// Use inverse distance weighting to estimate the temperature at Cape Canaveral on that day\n// This link explains more on inverse distance weighting: https://en.wikipedia.org/wiki/Inverse_distance_weighting (Use p=2 in the formula)\n\n// create inverse distance weight calculation function\ndef inv_dist_weight(dist: Double, p: Double): Double = {\n    1/(scala.math.pow(dist, p))\n}\nval weight = udf(inv_dist_weight _) // create udf\n\n// Calculate weights and apply to temp recordings with 100 miles on date of accident\nval stations_weight = stations_clean_near.withColumn(\"Weight\", weight($\"CapeCanaveralDist\", lit(2)))\nval near_temps = stations_weight.join(temps_0128, Seq(\"STATION\", \"WBAN\")).withColumn(\"weighted_temp\", $\"TEMP\" * $\"Weight\")","user":"anonymous","dateUpdated":"2020-04-08T17:51:15-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"inv_dist_weight: (dist: Double, p: Double)Double\nweight: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function2>,DoubleType,Some(List(DoubleType, DoubleType)))\nstations_weight: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 4 more fields]\nnear_temps: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 8 more fields]\n"}]},"apps":[],"jobName":"paragraph_1586286925679_1550633293","id":"20180319-091159_1767904139","dateCreated":"2020-04-07T15:15:25-0400","dateStarted":"2020-04-08T17:51:15-0400","dateFinished":"2020-04-08T17:51:16-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3086"},{"text":"// Once you have the weighted sum of temperatures (numerator) and the sum of the weights (denominator)\n// you can obtain your final result\n\nnear_temps.select(sum(\"weighted_temp\") / sum(\"Weight\")).show()","user":"anonymous","dateUpdated":"2020-04-08T17:52:05-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------------------------+\n|(sum(weighted_temp) / sum(Weight))|\n+----------------------------------+\n|                 36.98881742748224|\n+----------------------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1586286925679_206428961","id":"20180318-194257_301533474","dateCreated":"2020-04-07T15:15:25-0400","dateStarted":"2020-04-08T17:52:05-0400","dateFinished":"2020-04-08T17:52:11-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3087"},{"text":"// Extra credit - find average temperature for Jan 28 for every year.\n// Generate a line plot.\n// Was the Jan 28, 1986 temperature unusual?","user":"anonymous","dateUpdated":"2020-04-08T17:07:13-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1586286925680_-619646988","id":"20180318-194403_1746372836","dateCreated":"2020-04-07T15:15:25-0400","dateStarted":"2020-04-08T17:07:13-0400","dateFinished":"2020-04-08T17:07:13-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3088"}],"name":"DS4300_The_Challenger_Accident","id":"2F5XJU4DU","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}