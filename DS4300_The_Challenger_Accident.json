{"paragraphs":[{"text":"// Read temperature data for 1986 and station data\n\nval temps = spark.read.csv(\"/Users/arunima/Desktop/1986.csv\").toDF(\"STATION\", \"WBAN\", \"MONTH\", \"DAY\", \"TEMP\")\nval stations = spark.read.csv(\"/Users/arunima/Desktop/stations.csv\").toDF(\"STATION\", \"WBAN\", \"LAT\", \"LON\")","user":"anonymous","dateUpdated":"2020-04-07T15:31:30-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"temps: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 3 more fields]\nstations: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1586286925674_-1241852641","id":"20180318-191454_284618957","dateCreated":"2020-04-07T15:15:25-0400","dateStarted":"2020-04-07T15:31:30-0400","dateFinished":"2020-04-07T15:31:47-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5430"},{"text":"// Filter bad Stations where the latitude or longitude is unavailable\n// Only include temperature data for JANUARY 28th, 1986\n// You'll want to convert latitudes and longitudes to Doubles\n\nval stations_clean = stations.filter(($\"LAT\".isNotNull) && ($\"LON\".isNotNull)).selectExpr(\"STATION\", \"WBAN\", \"cast(LAT as double) LAT\", \"cast(LON as double) LON\")\nval temps_0128 = temps.filter(($\"MONTH\" === \"01\") && ($\"DAY\" === \"28\"))//.join(stations_clean,Seq(\"STATION\"))\n","user":"anonymous","dateUpdated":"2020-04-07T15:32:05-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"stations_clean: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 2 more fields]\ntemps_0128: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STATION: string, WBAN: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1586286925676_-1851971540","id":"20180318-191924_1310538952","dateCreated":"2020-04-07T15:15:25-0400","dateStarted":"2020-04-07T15:32:05-0400","dateFinished":"2020-04-07T15:32:08-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5431"},{"text":"// Define a function that compute the distance between two points on the Earth using the Haversine formula\n// https://www.movable-type.co.uk/scripts/latlong.html\n// Declare the function as a UDF (User-defined function) so that it can be applied\n\nval pi = 3.14159265\nval REarth = 6371.0 // kilometers\ndef toRadians(x: Double): Double = x * pi / 180.0\n\ndef haversine(lat1: Double, lon1: Double, lat2: Double, lon2: Double): Double = {\n    var φ1 = toRadians(lat1);\n    var φ2 = toRadians(lat2);\n    var Δφ = toRadians(lat2-lat1);\n    var Δλ = toRadians(lon2-lon1);\n\n    var a = Math.sin(Δφ/2) * Math.sin(Δφ/2) +\n            Math.cos(φ1) * Math.cos(φ2) *\n            Math.sin(Δλ/2) * Math.sin(Δλ/2);\n    var c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n\n    REarth * c;\n}\n\n\n// Now you can use \"haver\" as a function with Spark SQL \nimport org.apache.spark.sql.functions.udf\nval haver = udf(haversine _)","user":"anonymous","dateUpdated":"2020-04-07T15:32:33-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"pi: Double = 3.14159265\nREarth: Double = 6371.0\ntoRadians: (x: Double)Double\nhaversine: (lat1: Double, lon1: Double, lat2: Double, lon2: Double)Double\nimport org.apache.spark.sql.functions.udf\nhaver: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function4>,DoubleType,Some(List(DoubleType, DoubleType, DoubleType, DoubleType)))\n"}]},"apps":[],"jobName":"paragraph_1586286925677_-1437940270","id":"20180318-195342_1996330025","dateCreated":"2020-04-07T15:15:25-0400","dateStarted":"2020-04-07T15:32:33-0400","dateFinished":"2020-04-07T15:32:35-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5432"},{"text":"// Find all stations within 100 km using your haversine function\n\nval capeCanaveralLatitude = 28.388382\nval capeCanaveralLongitude = -80.603498\n\nval stations_clean_near = stations_clean.withColumn(\"CapeCanaveralDist\", haver($\"LAT\", $\"LON\", lit(capeCanaveralLatitude), lit(capeCanaveralLongitude))).filter($\"CapeCanaveralDist\" < 100)","user":"anonymous","dateUpdated":"2020-04-07T16:07:55-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"capeCanaveralLatitude: Double = 28.388382\ncapeCanaveralLongitude: Double = -80.603498\nstations_clean_near: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STATION: string, WBAN: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1586286925678_1759852908","id":"20180318-191955_1910278437","dateCreated":"2020-04-07T15:15:25-0400","dateStarted":"2020-04-07T16:07:55-0400","dateFinished":"2020-04-07T16:07:56-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5433"},{"text":"// Use inverse distance weighting to estimate the temperature at Cape Canaveral on that day\n// You might do this in serveral steps.  First compute a weight for each station within 100 km that recorded\n// a temperature.  Then, when you have a column of weights, apply an aggregation function to\n// multiply each station temperature by a weight and to compute the sum of the weights.\n// This link explains more on inverse distance weighting:\n// https://en.wikipedia.org/wiki/Inverse_distance_weighting\n// Use p=2 in the formula\n\n// create inverse distance weight calculation function\ndef inv_dist_weight(dist: Double, p: Double): Double = {\n    1/(scala.math.pow(dist, p))\n}\nval weight = udf(inv_dist_weight _) // create udf\n\nval stations_weight = stations_clean_near.withColumn(\"Weight\", weight($\"CapeCanaveralDist\", lit(2)))\n\n","user":"anonymous","dateUpdated":"2020-04-07T16:10:24-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"inv_dist_weight: (dist: Double, p: Double)Double\nweight: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function2>,DoubleType,Some(List(DoubleType, DoubleType)))\nstations_valid: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1586286925679_1550633293","id":"20180319-091159_1767904139","dateCreated":"2020-04-07T15:15:25-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5434","dateFinished":"2020-04-07T16:09:11-0400","dateStarted":"2020-04-07T16:09:11-0400"},{"text":"// Once you have the weighted sum of temperatures (numerator) and the sum of the weights (denominator)\n// you can obtain your final result\n\n","user":"anonymous","dateUpdated":"2020-04-07T15:15:25-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1586286925679_206428961","id":"20180318-194257_301533474","dateCreated":"2020-04-07T15:15:25-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5435"},{"text":"// Extra credit - find average temperature for Jan 28 for every year.\n// Generate a line plot.\n// Was the Jan 28, 1986 temperature unusual?","user":"anonymous","dateUpdated":"2020-04-07T15:15:25-0400","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1586286925680_-619646988","id":"20180318-194403_1746372836","dateCreated":"2020-04-07T15:15:25-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5436"}],"name":"DS4300_The_Challenger_Accident","id":"2F5XJU4DU","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}