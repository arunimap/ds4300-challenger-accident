{"paragraphs":[{"text":"// Read temperature data for 1986 and station data\n\nval temps = spark.read.csv(\"/Users/arunima/Desktop/ArunimaMAC/Education/Northeastern/Year_3/ds4300/HW/ds4300-challenger-accident/temperatures/1986.csv\").toDF(\"STATION\", \"WBAN\", \"MONTH\", \"DAY\", \"TEMP\")\nval stations = spark.read.csv(\"/Users/arunima/Desktop/ArunimaMAC/Education/Northeastern/Year_3/ds4300/HW/ds4300-challenger-accident/temperatures/stations.csv\").toDF(\"STATION\", \"WBAN\", \"LAT\", \"LON\")","user":"anonymous","dateUpdated":"2020-04-06T21:35:03-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"temps: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 3 more fields]\nstations: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1586205874515_-118647306","id":"20180318-191454_284618957","dateCreated":"2020-04-06T16:44:34-0400","dateStarted":"2020-04-06T21:35:03-0400","dateFinished":"2020-04-06T21:35:06-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:23633"},{"text":"// Filter bad Stations where the latitude or longitude is unavailable\n// Only include temperature data for JANUARY 28th, 1986\n// You'll want to convert latitudes and longitudes to Doubles\n\nval stations_clean = stations.filter(($\"LAT\".isNotNull) && ($\"LON\".isNotNull)).selectExpr(\"STATION\", \"WBAN\", \"cast(LAT as double) LAT\", \"cast(LON as double) LON\")\nval temps_0128 = temps.filter(($\"MONTH\" === \"01\") && ($\"DAY\" === \"28\"))//.join(stations_clean,Seq(\"STATION\"))\n","user":"anonymous","dateUpdated":"2020-04-06T21:21:23-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"stations_clean: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 2 more fields]\ntemps_0128: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STATION: string, WBAN: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1586205874542_-614382211","id":"20180318-191924_1310538952","dateCreated":"2020-04-06T16:44:34-0400","dateStarted":"2020-04-06T21:21:23-0400","dateFinished":"2020-04-06T21:21:24-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:23634"},{"text":"// Define a function that compute the distance between two points on the Earth using the Haversine formula\n// https://www.movable-type.co.uk/scripts/latlong.html\n// Declare the function as a UDF (User-defined function) so that it can be applied\n\nval pi = 3.14159265\nval REarth = 6371.0 // kilometers\ndef toRadians(x: Double): Double = x * pi / 180.0\n\ndef haversine(lat1: Double, lon1: Double, lat2: Double, lon2: Double): Double = {\n    var R = 6371e3; // metres\n    var φ1 = toRadians(lat1);\n    var φ2 = toRadians(lat2);\n    var Δφ = toRadians(lat2-lat1);\n    var Δλ = toRadians(lon2-lon1);\n\n    var a = Math.sin(Δφ/2) * Math.sin(Δφ/2) +\n            Math.cos(φ1) * Math.cos(φ2) *\n            Math.sin(Δλ/2) * Math.sin(Δλ/2);\n    var c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n\n    R * c;\n}\n\n\n// Now you can use \"haver\" as a function with Spark SQL \nimport org.apache.spark.sql.functions.udf\nval haver = udf(haversine _)","user":"anonymous","dateUpdated":"2020-04-06T21:43:22-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"pi: Double = 3.14159265\nREarth: Double = 6371.0\ntoRadians: (x: Double)Double\nhaversine: (lat1: Double, lon1: Double, lat2: Double, lon2: Double)Double\nimport org.apache.spark.sql.functions.udf\nhaver: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function4>,DoubleType,Some(List(DoubleType, DoubleType, DoubleType, DoubleType)))\n"}]},"apps":[],"jobName":"paragraph_1586205874544_-411372027","id":"20180318-195342_1996330025","dateCreated":"2020-04-06T16:44:34-0400","dateStarted":"2020-04-06T21:32:28-0400","dateFinished":"2020-04-06T21:32:30-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:23635"},{"text":"// Find all stations within 100 km using your haversine function\n\nval capeCanaveralLatitude = 28.388382\nval capeCanaveralLongitude = -80.603498\n\nval stations_clean_100 = stations_clean.filter(haver($\"LAT\", $\"LON\", capeCanaveralLatitude, capeCanaveralLongitude) < 100)","user":"anonymous","dateUpdated":"2020-04-06T21:58:44-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:33: error: type mismatch;\n found   : Double\n required: org.apache.spark.sql.Column\n       haver($\"LAT\", $\"LON\", capeCanaveralLatitude, capeCanaveralLongitude)\n                             ^\n<console>:33: error: type mismatch;\n found   : Double\n required: org.apache.spark.sql.Column\n       haver($\"LAT\", $\"LON\", capeCanaveralLatitude, capeCanaveralLongitude)\n                                                    ^\n"}]},"apps":[],"jobName":"paragraph_1586205874546_372070977","id":"20180318-191955_1910278437","dateCreated":"2020-04-06T16:44:34-0400","dateStarted":"2020-04-06T21:58:11-0400","dateFinished":"2020-04-06T21:58:11-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:23636"},{"text":"// Use inverse distance weighting to estimate the temperature at Cape Canaveral on that day\n// You might do this in serveral steps.  First compute a weight for each station within 100 km that recorded\n// a temperature.  Then, when you have a column of weights, apply an aggregation function to\n// multiply each station temperature by a weight and to compute the sum of the weights.\n// This link explains more on inverse distance weighting:\n// https://en.wikipedia.org/wiki/Inverse_distance_weighting\n// Use p=2 in the formula\n\n","user":"anonymous","dateUpdated":"2020-04-06T16:44:34-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1586205874547_-1702876902","id":"20180319-091159_1767904139","dateCreated":"2020-04-06T16:44:34-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23637"},{"text":"// Once you have the weighted sum of temperatures (numerator) and the sum of the weights (denominator)\n// you can obtain your final result\n\n","user":"anonymous","dateUpdated":"2020-04-06T16:44:34-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1586205874548_1634944997","id":"20180318-194257_301533474","dateCreated":"2020-04-06T16:44:34-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23638"},{"text":"// Extra credit - find average temperature for Jan 28 for every year.\n// Generate a line plot.\n// Was the Jan 28, 1986 temperature unusual?","user":"anonymous","dateUpdated":"2020-04-06T16:44:34-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1586205874549_60223156","id":"20180318-194403_1746372836","dateCreated":"2020-04-06T16:44:34-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23639"}],"name":"DS4300_The_Challenger_Accident","id":"2F4C12T88","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}